---
title: "summarizing CAM Data v03"
author: "Noah, Julius"
date: "01 04 2021"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: true
bibliography: LibraryAll.bib
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # define default chunk options


# sets the directory of location of this script as the current directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path)) 

################
# install and load packages
################
# wenn Pakete nicht bereits installiert sind, wird die Funktion diese installieren und aktivieren
usePackage <- function(p) {
  if (!is.element(p, installed.packages()[,1]))
    install.packages(p, dep = TRUE, repos = "http://cran.us.r-project.org")
  require(p, character.only = TRUE)
}
## Fehlerbehandlung Pakete in R base installieren
# options(repos="https://CRAN.R-project.org")

usePackage("tidyverse") # data cleaning and summarizing
usePackage("xlsx") # data cleaning and summarizing
usePackage("purr") # data cleaning and summarizing

## text mining / textprocessing
usePackage("SentimentAnalysis")
# usePackage("tm")
# usePackage("qdap")
## output tables
usePackage("stargazer") # Tabellen erstellen
# usePackage("Cairo") # Umgebung, um Grafiken zu speichern
## psychometric analysis
usePackage("psych") 

## meta analysis
usePackage("metafor")
usePackage("meta")

## save pictures
usePackage("Cairo")

################
# load data
################
study2canada <- xlsx::read.xlsx2(file = "data/study2canada_indicatorsCAMs_long.xlsx", sheetIndex = 1)
study2germany <- xlsx::read.xlsx2(file = "data/study2germany_indicatorsCAMs_long.xlsx", sheetIndex = 1)
study3 <- xlsx::read.xlsx2(file = "data/study3_indicatorsCAMs_long.xlsx", sheetIndex = 1)


study2canada <- as.data.frame(sapply(study2canada, as.numeric))
study2germany <- as.data.frame(sapply(study2germany, as.numeric))
study3 <- as.data.frame(sapply(study3, as.numeric))

```

```{r data preperation, include=FALSE}
## dummy variable for every study
study2canada$group <- "study2canada"
study2germany$group <- "study2germany"
study3$group <- "study3"
allstudies <- rbind(study2canada, study2germany, study3)

## remove variables
allstudies$centralityDummy_CoronaPandemie <- NULL 

## add variables
# percent of invaliddashed
allstudies$num_edges_invaliddashedpercent <- allstudies$num_edges_invaliddashed / allstudies$num_edges_dashed
allstudies$num_edges_invaliddashedpercent[allstudies$num_edges_invaliddashedpercent == 0] <- NA
# dummy of invaliddashed
allstudies$dummy_edges_invaliddashedpercen <- ifelse(is.na(allstudies$num_edges_invaliddashedpercent), yes = 0, no = 1)
```

```{r functions, include=FALSE}
## create descreptive tables: 
descreptivefunc <- function(datasets = list(study2canada, study2germany, study3), variable = NULL){
  vec_means <- rep(NA, times = length(datasets)) # c()
  vec_sds <- rep(NA, times = length(datasets))
  for(i in 1:length(datasets)){

    vec_means[i] <- mean(unlist(datasets[[i]][variable]), na.rm = TRUE)
    vec_sds[i] <- sd(unlist(datasets[[i]][variable]), na.rm = TRUE)
  }
  
  combineddataframe <- data.frame(studies = c("study2canada", "study2germany", "study3"),
             mean = vec_means,
             sd = vec_sds)
  
  return(combineddataframe)
}

## save pictures
# ?ggplot2::ggsave()
save_graphic <- function(filename){
  tmp <- paste(filename, ".png", sep = "")
  Cairo::Cairo(file=tmp,
               type="png",
               units="px",
               width=2500,
               height=1700,
               pointsize=44, #text is shrinking by saving graphic
               dpi= "auto",
               bg = "white")
}




## create ANOVA table: 
#!!! sensitive function to increasing groups
anovatable <- function(dat_all = allstudies, vec_AVs = NULL){
vec_vars <- c()
vec_Fvalue <- c()
vec_pvalue <- c()
vec_sigdiff <- c()

## des vectors:
vec_des_study2canada <- c()
vec_des_study2germany <- c()
vec_des_study3 <- c()
##
h = 1

for(i in 1:length(vec_AVs)){
tmp <- aov(dat_all[, vec_AVs[i]] ~ group, data=dat_all)

if(summary(tmp)[[1]][["Pr(>F)"]][1] < .05){
  # print(summary(tmp))
  vec_vars[h] <- vec_AVs[i]
  vec_Fvalue[h] <- summary(tmp)[[1]][["F value"]][1]
  vec_pvalue[h] <- summary(tmp)[[1]][["Pr(>F)"]][1]

  ## post hoc test (TukeyHSD)
  tmp_post <- TukeyHSD(tmp)
  vec_sigdiff[h] <- paste0(rownames(tmp_post$group)[tmp_post$group[,4] < .05], collapse = ", ")
  ##
  
  ## descreptive means and SD
  
vec_des_study2canada[h] <- paste0(round(x = tmp_des[1,2], digits = 2), " (", round(x = tmp_des[1,3], digits = 2),")")
vec_des_study2germany[h] <-  paste0(round(x = tmp_des[2,2], digits = 2), " (", round(x = tmp_des[2,3], digits = 2),")")
vec_des_study3[h] <- paste0(round(x = tmp_des[3,2], digits = 2), " (", round(x = tmp_des[3,3], digits = 2),")")
  ##
  h = h + 1
  }
}
  
outdat <- data.frame(AV = vec_vars, Fvalue = vec_Fvalue, pvalue = vec_pvalue, sigdiff = vec_sigdiff, meanSD_study2canada = vec_des_study2canada, meanSD_study2germany = vec_des_study2germany, meanSD_study3 = vec_des_study3)

  return(outdat)
}

anovatable(dat_all = allstudies, vec_AVs = colnames(allstudies)[c(2:24, 26)])
```


This is an [R Markdown](http://rmarkdown.rstudio.com) document. Instructions for writing these documents and background information can be found in the book [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/). When you execute code within the document, the results appear beneath the code. 

The aim of this document is to summarize CAM data over different studies. Therefore descreptive and metaanalytical procedures are applied. Fundamental sources for this procedures are [Doing Meta-Analysis in R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) and @borenstein2009meta

# Existing CAM data

* 2 COVID-19 Datensätze
* 1 Datensatz aus 2 Mas (David, Niklas)
* 1 Datensatz aus Ethikseminar
* 1 Datensatz aus Mediationsdesign
* (1 Datensatz Lebens-CAMs) - in Klammern da anderes Konstruktionsprinzip, da angeleitet und von ExpertInnen

# Violations of CAM rules
incompatible connections... @thagard2010empathica
```{r nodes13, results='markup'}
# Invalid dashed (negative -- negative)



table2_nona <- allstudies %>%
  group_by(group) %>%
  filter(!is.na(num_edges_invaliddashedpercent)) %>%
  summarise(mean(num_edges_invaliddashedpercent), n())

table2_withna <- allstudies %>%
  group_by(group) %>%
  summarise(sum(is.na(num_edges_invaliddashedpercent)))

table2 <- cbind(table2_nona, table2_withna)


table2 <- allstudies %>%
        group_by(group) %>%
        summarise("Mean of invalid dashed %" = sprintf("%0.2f", mean(num_edges_invaliddashedpercent * 100, na.rm = TRUE)),
                  "number invalid dashed" = sum(!is.na(num_edges_invaliddashedpercent)),
                  "number of CAMs" = n())

table2
# table2 %>% stargazer(out = "aa.html", type = "html", summary = FALSE)

table_whyisdashedhigh_mean <- allstudies %>%
 group_by(group, dummy_edges_invaliddashedpercen) %>%
  select(!(CAM_ID:assortativity_valence:assortativityDegree)) %>%
  summarise_if(is.numeric, mean)
table_whyisdashedhigh_mean
# table_whyisdashedhigh_mean %>% stargazer(out = "bb.html", type = "html", summary = FALSE)

  table_whyisdashedhigh_sd <- allstudies %>%
 group_by(group, dummy_edges_invaliddashedpercen) %>%
  select(!(CAM_ID:assortativity_valence:assortativityDegree)) %>%
  summarise_if(is.numeric, sd)
table_whyisdashedhigh_sd




```


# apply semantic dictionaries
correlation between valence of single nodes and dictionary


# descreptive statististics of network indicators

## number of nodes and edges
```{r nodes1, results='asis'}
descreptivefunc(variable = "num_nodes") %>% 
  stargazer(summary = FALSE, type = "html", digits = 2)
```


```{r nodes2, results='markup'}
descreptivefunc(variable = "num_nodes")
```

```{r nodes3, results='markup'}
allstudies %>%
  group_by(group) %>% 
  summarise(mean(meanDistance_directed))

allstudies %>% 
  group_by(group) %>% 
  summarise(mean(num_nodes))

allstudies %>% 
  group_by(group) %>% 
  summarise(mean(mean_valence), median(mean_valence), sd(mean_valence))

boxplot(allstudies$num_edges_dashed)


# Ideen:
# Percentage of nodes other than 0 -solved
# If more node variation then X more often?
# Falls mean von num_nodes gering ausfällt, fallen dann auch andere nodes geringer aus?
# Hat mean_valence einen Einfluss auf X?
# num_nodes_pos minus num_nodes_neg
# Wenn durchschnittlich mehr nodes auch gleich durchschnittlich mehr num_edges? -solved
# Welche statistischen Tests evtl. erheben?
# Welche Werte sind besonders wichtig, diese zu vergleichen?
# Ich habe bisher viel mit mean gearbeitet, davon wegkommen?

allstudies %>%
  group_by(group) %>%
  count(num_nodes_ambi)

# Percentage of nodes other than 0

# building a table (wip)

nodes_ambi_bigger0 <- with(allstudies, c(sum(num_nodes_ambi > 0)) / nrow(allstudies))

nodes_neut_bigger0 <- with(allstudies, c(sum(num_nodes_neut > 0)) / nrow(allstudies))
                             
nodes_plot <- data.frame(nodes_ambi_bigger0, nodes_neut_bigger0)

# If more node variation then X more often?

# num_nodes_pos minus num_nodes_neg

allstudies %>% 
  group_by(group) %>% 
  summarise(mean(num_nodes_pos - num_nodes_neg))

# Wenn durchschnittlich mehr nodes auch gleich durchschnittlich mehr num_edges?

with(allstudies, c(sum(num_nodes > num_edges)))
paste0(allstudies$group[with(allstudies, num_nodes > num_edges)], allstudies$CAM_ID[with(allstudies, num_nodes > num_edges)])


with(allstudies, c(sum(num_nodes < num_edges)))
with(allstudies, c(sum(mean(num_nodes) > num_edges)))
with(allstudies, c(sum(mean(num_nodes) < num_edges)))

# Invalid dashed

allstudies$num_edges_invaliddashedpercent <- allstudies$num_edges_invaliddashed / allstudies$num_edges_dashed

table2_nona <- allstudies %>%
  group_by(group) %>%
  filter(!is.na(num_edges_invaliddashedpercent)) %>%
  summarise(mean(num_edges_invaliddashedpercent), n())

table2_withna <- allstudies %>%
  group_by(group) %>%
  summarise(sum(is.na(num_edges_invaliddashedpercent)))

table2 <- cbind(table2_nona, table2_withna)

table2 <- allstudies %>%
        group_by(group) %>%
        summarise(mean_invaliddashedpercent = sprintf("%0.2f", mean(num_edges_invaliddashedpercent * 100, na.rm = TRUE)),
                  N = n(),
                  NAs = sum(is.na(num_edges_invaliddashedpercent)))

table2

table_whyisdashedhigh <- allstudies %>%
  group_by(group) %>%
  filter(!is.na(num_edges_invaliddashedpercent)) %>%
  select(!(CAM_ID:assortativity_valence:assortativityDegree)) %>%
  summarise_if(is.numeric, mean)

table_whyisdashedhigh

# diameter_weighted low -> num_edges_invaliddashedpercent low (study3)
# mean_valence_normed high -> num_edges_invaliddashedpercent low
# study3 nur 11% invalid: diameter_weighted lowest, mean_valence_normed highest, meanWeightEdges lowest, mean_valence_normed lowest, mean_valence lowest
# was dagegen spricht, study2germany 20% invalid: diameter_weighted highest
# korrelation zwischen: je höher/niedriger mean_valence(_normed) desto mehr/geringer fällt num_edges_invaliddashedpercent aus

cor.test(table_whyisdashedhigh$mean_valence, table_whyisdashedhigh$num_edges_invaliddashedpercent)

# cor.test(table_whyisdashedhigh$mean_valence[1], table_whyisdashedhigh$num_edges_invaliddashedpercent[1])

# cor.test(table_whyisdashedhigh$mean_valence[2], table_whyisdashedhigh$num_edges_invaliddashedpercent[2])

# cor.test(table_whyisdashedhigh$mean_valence[3], table_whyisdashedhigh$num_edges_invaliddashedpercent[3])

# Lineare Regression

linearer <- lm(num_edges ~ num_nodes, data=allstudies)

summary(linearer)

plot(allstudies$num_edges, allstudies$num_nodes, xlab="Predictor", ylab="Outcome", col="darkblue", pch=16, main="Simple Linear Regression")

for (i in 1:length(linearer$fitted.values))
  lines(c(allstudies$num_edges[i], allstudies$num_edges[i]), c(allstudies$num_nodes[i], linearer$fitted.values[i]), col="blue")

abline(linearer, col="darkred")

scatter.smooth(x=allstudies$num_edges, y=allstudies$num_nodes, main="Edges ~ Nodes") 

```


# correlations of network indicators


# topology of CAMs
Blah blah [see @MAkreil2018staircaseelevator, pp. 20ff.]...


# additionally: feedback to CAM software


# additionally: comments to single nodes

# Julius code
```{r randomcode 1, results='markup'}
allstudies %>%
  mutate(baseline = mean(num_nodes), baselineboolean = as.numeric(num_nodes < baseline)) %>%
  group_by(group, baselineboolean) %>%
  summarise(mean(num_nodes_pos))
```

```{r randomcode 2, results='markup'}
psych::cor.plot(r = cor(allstudies[, 2:23], use = "pairwise.complete.obs"), upper = FALSE, xlas = 2)
# allstudies$num_edges_invaliddashedpercent <- allstudies$num_edges_invaliddashed / allstudies$num_edges_dashed

# save_graphic(filename = "network indicators all datasets")
# psych::cor.plot(r = cor(allstudies[, 2:23], use = "pairwise.complete.obs"), upper = FALSE, xlas = 2)
# dev.off()
psych::cor.plot(r = cor(allstudies[, c(2:24, 26)], use = "pairwise.complete.obs"), upper = FALSE, xlas = 2)
plot(allstudies$num_edges_invaliddashedpercent, allstudies$assortativity_valence)
```


# Random notes

# delete a directory -- must add recursive = TRUE
# unlink("some_directory", recursive = TRUE)
# wieder löschen
# see: http://theautomatic.net/2018/07/11/manipulate-files-r/


# References
